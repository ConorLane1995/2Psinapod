"""
TODO doc
"""


import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from scipy.ndimage.filters import gaussian_filter1d
from matplotlib.lines import Line2D
from scipy.io import loadmat
import seaborn as sns
import pickle
import json
import sys
import os

sys.path.append(os.path.abspath(os.path.dirname(__file__)) + '/../../')
from utils import get_active_cells

# load what we need from the config file
with open(os.path.abspath(os.path.dirname(__file__)) +'/../../../config.json','r') as f:
    config = json.load(f)

BASE_PATH = config['RecordingFolder'] # folder with all of the files generated by Suite2P for this recording (F.npy, iscell.npy, etc)
CONDITIONS_PATH = config['Conditions'] # name of the CSV (assumed to be within BASE_PATH) with the condition types of each trial (freq, intensity, etc)
CELL_DICT = config['AnalysisFile'] # name of the file that all of the analysis is getting saved in (tuning, best frequency, etc)

ACTIVE_CELLS_ONLY = True
N_BASELINE_FRAMES = 4


# more info: https://pietromarchesi.net/pca-neural-data.html

def reformat_epoched_data(data):
    # taking in nNeurons x nTrials x nFrames array
    # passing out nNeurons x (nTrials*nFrames) array

    n_by_f_data = np.reshape(data,(len(data),len(data[0])*len(data[0][0])))
    return n_by_f_data

def standardize(data):
    # data: n_features x n_samples

    ss = StandardScaler(with_mean=True,with_std=True)
    data_c = ss.fit_transform(data.T).T
    return data_c

def re_epoch_data(data,n_trials):
    nxtxf_data = np.reshape(data,(len(data),n_trials,-1))
    return nxtxf_data



shade_alpha      = 0.2
lines_alpha      = 0.8
pal = sns.color_palette('rocket', 13)


def add_stim_to_plot(ax):
    start_stim = 4
    end_stim = 14
    ax.axvspan(start_stim, end_stim, alpha=shade_alpha,
               color='gray')
    ax.axvline(start_stim, alpha=lines_alpha, color='gray', ls='--')
    ax.axvline(end_stim, alpha=lines_alpha, color='gray', ls='--')
    
def add_orientation_legend(ax,trial_types):
    custom_lines = [Line2D([0], [0], color=pal[k], lw=4) for
                    k in range(len(trial_types))]
    labels = ['{}'.format(t) for t in trial_types]
    ax.legend(custom_lines, labels,
              frameon=False, loc='center left', bbox_to_anchor=(1, 0.5))
    plt.tight_layout(rect=[0,0,0.9,1])



def main():
    data = np.load(BASE_PATH+"epoched_traces.npy")
    conditions_mat = loadmat(BASE_PATH+CONDITIONS_PATH) # conditition type of each trial in chronological order (row 1 = trial 1)
    conditions = conditions_mat["stim_data"]

    # load the recording info file
    with open(BASE_PATH + "recording_info.pkl","rb") as f:
        recording_info = pickle.load(f)

    if ACTIVE_CELLS_ONLY:
        with open(BASE_PATH+CELL_DICT,"rb") as f:
            cell_dict = pickle.load(f)

        active_cell_dict = get_active_cells(cell_dict)
        active_cell_IDs = np.array(list(active_cell_dict.keys()))
        all_IDs = np.array(list(cell_dict.keys()))
        active_cell_idx = np.nonzero(np.in1d(all_IDs,active_cell_IDs))[0]

        epoched_data = data[active_cell_idx,:,:]
    else:
        epoched_data = data

    # trials is a list of K Numpy arrays of shape NÃ—T (number of neurons by number of time points)
    trials = []
    for trial_idx in range(len(epoched_data[0])):
        trials.append(epoched_data[:,trial_idx,:])

    trial_type = conditions[:,0] # list of length K containing the type (i.e. frequency) of every trial

    trial_types = np.unique(conditions[:,0]) # list containing the unique trial types (i.e. frequencies) in ascending order

    # get all the indices for a single trial type, for every trial type
    t_type_ind = [np.argwhere(np.array(trial_type) == t_type)[:, 0] for t_type in trial_types]

    # average all of the trials of a single type together
    trial_averages = []
    for ind in t_type_ind:
        trial_averages.append(np.array(trials)[ind].mean(axis=0))
    Xa = np.hstack(trial_averages)

    Xa = standardize(Xa) 
    pca = PCA(n_components=15)
    Xa_p = pca.fit_transform(Xa.T).T

    trial_size   = trials[0].shape[1]

    fig, axes = plt.subplots(1, 3, figsize=[10, 2.8], sharey='row')
    for comp in range(3):
        ax = axes[comp]
        for kk,_ in enumerate(trial_types):
            x = Xa_p[comp, kk * trial_size :(kk+1) * trial_size]
            x = gaussian_filter1d(x, sigma=3)
            ax.plot(range(15), x, c=pal[kk])
        add_stim_to_plot(ax)
        ax.set_ylabel('PC {}'.format(comp+1))
    add_orientation_legend(axes[2],trial_types)
    axes[1].set_xlabel('Time (s)')
    sns.despine(fig=fig, right=True, top=True)
    plt.tight_layout(rect=[0, 0, 0.9, 1])

    # find the indices of the three largest elements of the second eigenvector
    units = np.abs(pca.components_[1, :].argsort())[::-1][0:3]

    f, axes = plt.subplots(1, 3, figsize=[10, 2.8], sharey=False,
                        sharex=True)
    for ax, unit in zip(axes, units):
        ax.set_title('Neuron {}'.format(unit))
        for t, ind in enumerate(t_type_ind):
            x = np.array(trials)[ind][:, unit, :]
            sns.lineplot(x=range(15),y=np.mean(x,axis=0),
                    ax=ax,
                    err_style='band',
                    ci="sd",
                    color=pal[t])
    for ax in axes:
        add_stim_to_plot(ax)
    axes[1].set_xlabel('Time (s)')
    sns.despine(fig=f, right=True, top=True)
    add_orientation_legend(axes[2],trial_types)
    print(pca.explained_variance_ratio_)
    plt.show()




if __name__=="__main__":
    main()